# BreezeApp Runner é–‹ç™¼æŒ‡å—

ä¸€ä»½åœ¨ BreezeApp-engine ä¸­å»ºç«‹è‡ªè¨‚ AI runner çš„ç°¡å–®æŒ‡å—ã€‚

## å¿«é€Ÿå…¥é–€

### 1. è¤‡è£½ç¯„æœ¬

åœ¨ `android/breeze-app-engine/src/main/java/com/mtkresearch/breezeapp/engine/runner` ç›®éŒ„ä¸‹ï¼Œ

```bash
cp templates/CustomRunner.kt yourvendor/YourRunner.kt
```

### 2. æ›´æ–°è£é£¾è©

```kotlin
@AIRunner(
    vendor = VendorType.UNKNOWN,           // é¸æ“‡æ‚¨çš„ä¾›æ‡‰å•†
    priority = RunnerPriority.NORMAL,      // HIGH/NORMAL/LOW  
    capabilities = [CapabilityType.LLM],   // LLM/ASR/TTS/VLM/GUARDIAN
    defaultModel = "your-model-name" // å°æ–¼æœ¬åœ°æ¨¡å‹ï¼Œè«‹èˆ‡ fullModelList.json ä¸­çš„ 'id' å°é½Šï¼›å°æ–¼é›²ç«¯æ¨¡å‹ï¼Œè«‹èˆ‡ API åç¨±å°é½Šã€‚
)
class YourRunner : BaseRunner, FlowStreamingRunner {
    // å¯¦ä½œ...
}
```

### 3. å¯¦ä½œæ‚¨çš„ AI é‚è¼¯

```kotlin
// `BaseRunner` ä¸­çš„ `run` æ–¹æ³•å’Œ `FlowStreamingRunner` ä¸­çš„ `runAsFlow` æ˜¯æ‚¨çš„ runner æ¨è«–é‚è¼¯çš„ä¸»è¦å…¥å£é»ã€‚
// é€™æ˜¯æ‚¨çš„ runner æ¥æ”¶ `InferenceRequest` ä¸¦è¿”å› `InferenceResult` çš„åœ°æ–¹ã€‚

// `run` æ–¹æ³•å¯¦ä½œç¯„ä¾‹
override fun run(request: InferenceRequest): InferenceResult {
    // è¼¸å…¥è³‡æ–™ï¼ˆæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ï¼‰ä½æ–¼ `request.inputs` ä¸­ã€‚
    val inputText = request.inputs[InferenceRequest.INPUT_TEXT] as? String ?: ""

    // æ‚¨çš„æ ¸å¿ƒ AI é‚è¼¯ï¼ˆä¾‹å¦‚ï¼Œå‘¼å« API æˆ–æœ¬åœ°æ¨¡å‹ï¼‰
    val aiResponse = apiClient.generateText(inputText)

    // è¼¸å‡ºæ‡‰é€é `InferenceResult.textOutput()`ã€`InferenceResult.audioOutput()` ç­‰æ–¹æ³•è¿”å›ã€‚
    return InferenceResult.textOutput(text = aiResponse)
}

// å°æ–¼ä¸²æµ runnerï¼Œè«‹å¯¦ä½œ `runAsFlow`ï¼ˆè«‹åƒé–±ç¯„æœ¬ä»¥å–å¾—ç¯„ä¾‹ï¼‰ã€‚
// `runAsFlow` æ–¹æ³•å¯¦ä½œç¯„ä¾‹ï¼š
override fun runAsFlow(request: InferenceRequest): Flow<InferenceResult> = flow {
    val inputText = request.inputs[InferenceRequest.INPUT_TEXT] as? String ?: ""
    var accumulatedText = ""

    // æ¨¡æ“¬ä¸²æµè¼¸å‡º
    apiClient.streamGenerate(inputText) { chunk ->
        accumulatedText += chunk
        emit(InferenceResult.textOutput(text = accumulatedText, partial = true))
    }
    // ç™¼å‡ºæœ€çµ‚çµæœ
    emit(InferenceResult.textOutput(text = accumulatedText, partial = false))
}
```

æœ‰é—œæ›´è©³ç´°çš„ä¸²æµæ¨¡å¼å’Œæœ€ä½³å¯¦è¸ï¼Œè«‹åƒé–±ï¼š[ä¸²æµå¯¦ä½œæŒ‡å—](../android/breeze-app-engine/src/main/java/com/mtkresearch/breezeapp/engine/runner/STREAMING_GUIDE.md)

**å°±æ˜¯é€™æ¨£ï¼** å¼•æ“æœƒè‡ªå‹•ç™¼ç¾ä¸¦æ•´åˆæ‚¨çš„ runnerã€‚

## ç¯„ä¾‹

### LLM Runner ç¯„ä¾‹

```kotlin
@AIRunner(vendor = VendorType.OPENROUTER, capabilities = [CapabilityType.LLM])
class MyLLMRunner : BaseRunner {
    override fun run(input: InferenceRequest, stream: Boolean): InferenceResult {
        return try {
            val text = input.inputs[InferenceRequest.INPUT_TEXT] as? String ?: ""
            val response = apiClient.generateText(text)
            InferenceResult.success(mapOf("text" to response))
        } catch (e: Exception) {
            InferenceResult.error(RunnerError.processingError("Generation failed: ${e.message}", e))
        }
    }
}
```

### ASR Runner ç¯„ä¾‹

```kotlin
@AIRunner(vendor = VendorType.SHERPA, capabilities = [CapabilityType.ASR])
class MyASRRunner : BaseRunner {
    override fun run(input: InferenceRequest, stream: Boolean): InferenceResult {
        return try {
            val audio = input.inputs[InferenceRequest.INPUT_AUDIO] as? ByteArray ?: byteArrayOf()
            if (audio.isEmpty()) {
                return InferenceResult.error(RunnerError.invalidInput("Audio input is required"))
            }
            val transcript = asrClient.transcribe(audio)
            InferenceResult.success(mapOf("text" to transcript))
        } catch (e: Exception) {
            InferenceResult.error(RunnerError(RunnerError.Code.ASR_FAILURE, "Transcription failed: ${e.message}", e))
        }
    }
}
```

### TTS Runner ç¯„ä¾‹

```kotlin
@AIRunner(vendor = VendorType.SHERPA, capabilities = [CapabilityType.TTS])
class MyTTSRunner : BaseRunner {
    override fun run(input: InferenceRequest, stream: Boolean): InferenceResult {
        return try {
            val text = input.inputs[InferenceRequest.INPUT_TEXT] as? String ?: ""
            if (text.isBlank()) {
                return InferenceResult.error(RunnerError.invalidInput("Text input cannot be empty"))
            }
            val audioData = ttsClient.synthesize(text)
            InferenceResult.success(mapOf("audio_data" to audioData, "sample_rate" to 22050))
        } catch (e: Exception) {
            InferenceResult.error(RunnerError(RunnerError.Code.TTS_FAILURE, "Speech synthesis failed: ${e.message}", e))
        }
    }
}
```

## éŒ¯èª¤è™•ç†

å§‹çµ‚ä½¿ç”¨ `RunnerError` å·¥å» æ–¹æ³•è¿”å›çµæ§‹åŒ–éŒ¯èª¤ä»¥ä¿æŒä¸€è‡´æ€§ã€‚

```kotlin
// è¼¸å…¥é©—è­‰éŒ¯èª¤
return InferenceResult.error(RunnerError.invalidInput("Text input cannot be empty"))

// è™•ç†éŒ¯èª¤
return InferenceResult.error(RunnerError.processingError("API call failed: ${e.message}", e))

// è³‡æºéŒ¯èª¤
return InferenceResult.error(RunnerError.resourceUnavailable("Model not loaded"))
```

**éŒ¯èª¤ç¢¼æŒ‡å—ï¼š**

éŒ¯èª¤ç¢¼é›†ä¸­åœ¨ `RunnerError.Code` ä¸­ä»¥ç¢ºä¿ä¸€è‡´æ€§ã€‚ç›¡å¯èƒ½ä½¿ç”¨å·¥å» æ–¹æ³•ï¼ˆ`RunnerError.invalidInput(...)`ã€`RunnerError.processingError(...)` ç­‰ï¼‰ã€‚

- **E1xx**ï¼šè™•ç†éŒ¯èª¤ï¼ˆä¾‹å¦‚ï¼Œæ¨è«–å¤±æ•—ï¼‰
- **E4xx**ï¼šå®¢æˆ¶ç«¯/è¼¸å…¥éŒ¯èª¤ï¼ˆä¾‹å¦‚ï¼Œç„¡æ•ˆåƒæ•¸ã€æ¬Šé™ï¼‰
- **E5xx**ï¼šæœå‹™å™¨/è³‡æºéŒ¯èª¤ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹è¼‰å…¥ã€è³‡æºä¸å¯ç”¨ï¼‰

è©³ç´°éŒ¯èª¤ç¢¼å®šç¾©åœ¨ `engine/model/RunnerError.kt` ä¸­ã€‚

## é—œéµè¦é»

âœ… **è‡ªå‹•ç™¼ç¾** - è‡ªå‹•ç™¼ç¾
âœ… **åƒæ•¸ UI** - è‡ªå‹•è¨­å®š UI
âœ… **ä¸²æµ** - ä¸²æµ
âœ… **éŒ¯èª¤è™•ç†** - éŒ¯èª¤è™•ç†
âœ… **è¨˜æ†¶é«”ç®¡ç†** - è¨˜æ†¶é«”ç®¡ç†

## éœ€è¦å¹«åŠ©å—ï¼Ÿ

- ğŸ“‹ **å¾ç¯„æœ¬é–‹å§‹** - `templates/CustomRunner.kt`
- ğŸš€ **ä¸²æµæ¨¡å¼** - `runner/STREAMING_GUIDE.md`
- ğŸ“ **å¯¦éš›ç¯„ä¾‹** - `executorch/`, `openrouter/`, `sherpa/`, `mock/`
- ğŸ§ª **æ¸¬è©¦æ¨¡å¼** - `src/test/`

**å°ˆæ³¨æ–¼æ‚¨çš„ AI é‚è¼¯ - å¼•æ“è™•ç†å‰©ä¸‹çš„éƒ¨åˆ†ï¼**
