# BreezeApp Runner é–‹ç™¼æŒ‡å—

ä¸€ä»½åœ¨ BreezeApp-engine ä¸­å»ºç«‹è‡ªè¨‚ AI runner çš„ç°¡å–®æŒ‡å—ã€‚

## å¿«é€Ÿå…¥é–€

### 1. è¤‡è£½ç¯„æœ¬
```bash
cp templates/CustomRunner.kt yourvendor/YourRunner.kt
```

### 2. æ›´æ–°è¨»è§£
```kotlin
@AIRunner(
    vendor = VendorType.UNKNOWN,           // é¸æ“‡æ‚¨çš„ä¾›æ‡‰å•†
    priority = RunnerPriority.NORMAL,      // HIGH/NORMAL/LOW  
    capabilities = [CapabilityType.LLM],   // LLM/ASR/TTS/VLM/GUARDIAN
    defaultModel = "your-model-name"
)
class YourRunner : BaseRunner, FlowStreamingRunner {
    // å¯¦ä½œ...
}
```

### 3. å¯¦ä½œæ‚¨çš„ AI é‚è¼¯
```kotlin
// å°æ–¼ LLM:
private fun processTextInput(text: String): String {
    return apiClient.generateText(text)
}

// å°æ–¼ ASR:
private fun processAudioInput(audio: ByteArray): String {
    return apiClient.transcribeAudio(audio)
}

// å°æ–¼ TTS, VLM, GUARDIAN - è«‹åƒé–±ç¯„æœ¬ä»¥å–å¾—ç¯„ä¾‹
```

## ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šMTK NPU Runner
```kotlin
@AIRunner(
    vendor = VendorType.MEDIATEK,
    priority = RunnerPriority.HIGH,
    capabilities = [CapabilityType.LLM]
)
class MTKLLMRunner(private val context: Context? = null) : BaseRunner, FlowStreamingRunner {
    
    override fun load(): Boolean {
        val defaultConfig = ModelConfig(
            modelName = MODEL_NAME,
            modelPath = "" // ç©ºç™½ - ä½¿ç”¨ MTKUtils.resolveModelPath()
        )
        return load(defaultConfig)
    }
    
    // æ™ºæ…§æ¨¡å‹è·¯å¾‘è§£æ
    override fun load(config: ModelConfig): Boolean {
        val modelPath = if (context != null) {
            MTKUtils.resolveModelPath(context, config.modelPath)
        } else {
            config.modelPath ?: return false
        }
        // ... MTK åˆå§‹åŒ–
    }
}
```

### ç¯„ä¾‹ 2ï¼šMock Runner (çµ•ä½³ç¯„æœ¬)
```kotlin
@AIRunner(
    vendor = VendorType.UNKNOWN,
    priority = RunnerPriority.LOW,
    capabilities = [CapabilityType.LLM]
)
class MockLLMRunner : BaseRunner, FlowStreamingRunner {
    
    override fun load(): Boolean {
        val defaultConfig = ModelConfig(
            modelName = "MockLLMModel",
            modelPath = ""
        )
        return load(defaultConfig)
    }
    
    override fun run(input: InferenceRequest, stream: Boolean): InferenceResult {
        val inputText = input.inputs[InferenceRequest.INPUT_TEXT] as? String ?: ""
        val response = "Mock AI å›æ‡‰: $inputText"
        
        return InferenceResult.textOutput(
            text = response,
            metadata = mapOf(
                InferenceResult.META_MODEL_NAME to "mock-llm-v1",
                InferenceResult.META_PROCESSING_TIME_MS to 100L
            )
        )
    }
}
```

## æ¸¬è©¦

æ‚¨çš„ runner æœƒç«‹å³ç”Ÿæ•ˆï¼š
```kotlin
// å¼•æ“æœƒè‡ªå‹•å°‹æ‰¾ä¸¦ä½¿ç”¨æ‚¨çš„ runner
val request = InferenceRequest(inputs = mapOf("text" to "Hello"))
val result = engineManager.runInference(request, CapabilityType.LLM)
```

## é—œéµè¦é»

âœ… **è‡ªå‹•ç™¼ç¾** - ç„¡éœ€è¨»å†Š  
âœ… **åƒæ•¸ UI** - å®šç¾© schema ä»¥è‡ªå‹•ç”Ÿæˆè¨­å®š UI  
âœ… **ä¸²æµ** - å¯¦ä½œ `FlowStreamingRunner` ä»¥å¯¦ç¾å³æ™‚å›æ‡‰  
âœ… **éŒ¯èª¤è™•ç†** - ä½¿ç”¨ `RunnerError` é€²è¡Œçµæ§‹åŒ–éŒ¯èª¤è™•ç†  
âœ… **è¨˜æ†¶é«”ç®¡ç†** - å¼•æ“è™•ç†è¼‰å…¥/å¸è¼‰  

## å¸¸è¦‹æ¨¡å¼

### ç¡¬é«”åµæ¸¬
```kotlin
companion object : BaseRunnerCompanion {
    @JvmStatic
    override fun isSupported(): Boolean {
        return try {
            // æª¢æŸ¥åŸç”Ÿå‡½å¼åº«
            System.loadLibrary("your-ai-lib")
            
            // æª¢æŸ¥è£ç½®èƒ½åŠ›  
            val hasEnoughMemory = Runtime.getRuntime().maxMemory() > 4_000_000_000L
            
            // æª¢æŸ¥ API å¯ç”¨æ€§
            YourAILibrary.isAvailable()
            
            hasEnoughMemory
        } catch (e: Exception) {
            false
        }
    }
}
```

### è¼¸å…¥/è¼¸å‡ºè™•ç†
```kotlin
// å¸¸è¦‹è¼¸å…¥éµ
val text = input.inputs[InferenceRequest.INPUT_TEXT] as? String
val audio = input.inputs[InferenceRequest.INPUT_AUDIO] as? ByteArray
val image = input.inputs[InferenceRequest.INPUT_IMAGE] as? ByteArray

// å¸¸è¦‹åƒæ•¸  
val temperature = (input.params[InferenceRequest.PARAM_TEMPERATURE] as? Number)?.toFloat() ?: 0.7f
val maxTokens = (input.params[InferenceRequest.PARAM_MAX_TOKENS] as? Number)?.toInt() ?: 2048

// å›å‚³çµæœ
return InferenceResult.textOutput(
    text = response,
    metadata = mapOf(
        InferenceResult.META_MODEL_NAME to "your-model",
        InferenceResult.META_PROCESSING_TIME_MS to processingTime
    )
)
```

## å¯ç”¨é¡å‹

**èƒ½åŠ› (Capabilities):**
- `LLM` - æ–‡å­—ç”Ÿæˆ (æ–‡å­— â†’ æ–‡å­—)
- `ASR` - èªéŸ³è¾¨è­˜ (éŸ³è¨Š â†’ æ–‡å­—)
- `TTS` - æ–‡å­—è½‰èªéŸ³ (æ–‡å­— â†’ éŸ³è¨Š)
- `VLM` - è¦–è¦º + èªè¨€ (æ–‡å­— + åœ–ç‰‡ â†’ æ–‡å­—)
- `GUARDIAN` - å…§å®¹å®‰å…¨ (æ–‡å­— â†’ å®‰å…¨æ€§åˆ†æ)

**ä¾›æ‡‰å•† (Vendors):**
- `OPENROUTER` - é›²ç«¯ API æœå‹™
- `EXECUTORCH` - æœ¬åœ°è¡Œå‹•è£ç½®æ¨ç†
- `SHERPA` - åŸºæ–¼ ONNX çš„è™•ç†
- `MEDIATEK` - NPU åŠ é€Ÿ
- `UNKNOWN` - è‡ªè¨‚/æœªæŒ‡å®š

## éœ€è¦å”åŠ©ï¼Ÿ

- ğŸ“‹ **å¾ç¯„æœ¬é–‹å§‹** - `templates/CustomRunner.kt` 
- ğŸš€ **ä¸²æµæ¨¡å¼** - `runner/STREAMING_GUIDE.md` ä»¥å–å¾—è©³ç´°çš„ä¸²æµå¯¦ä½œ
- ğŸ“ **çœŸå¯¦ç¯„ä¾‹** - `executorch/`, `openrouter/`, `sherpa/`, `mock/` ç›®éŒ„
- ğŸ§ª **æ¸¬è©¦æ¨¡å¼** - `src/test/` ä»¥å–å¾—ä½¿ç”¨ç¯„ä¾‹

**å°ˆæ³¨æ–¼æ‚¨çš„ AI é‚è¼¯ - å¼•æ“æœƒè™•ç†å…¶é¤˜éƒ¨åˆ†ï¼**
